{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHdYgj7gJfwj",
        "outputId": "60b1a787-08c3-4704-fb78-cccc50c274ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m30.7/40.9 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m600.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.7/462.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.9/210.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.2/210.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.6/327.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.5/288.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.1/170.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qqq install 'crewai[tools]'\n",
        "!pip -qqq install youtube-transcript-api\n",
        "!pip -qqq install yt_dlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from crewai import Agent\n",
        "from google.colab import userdata\n",
        "from crewai import Crew, Process\n",
        "from crewai_tools import YoutubeChannelSearchTool\n",
        "from crewai import Task"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hogpbTnLJ3VL",
        "outputId": "89590a51-c861-4aca-ef32-77807568633e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
            "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
            "* 'smart_union' has been removed\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = userdata.get('OPEN_AI_KEY')\n",
        "model_ID = userdata.get('GPT_MODEL')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"OPENAI_MODEL_NAME\"] = model_ID"
      ],
      "metadata": {
        "id": "CMobxDfcJmeA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The tool used by the topic researcher\n",
        "youtube_tool = YoutubeChannelSearchTool(youtube_channel_handle='@techwithzoum')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzglqkb-KHcj",
        "outputId": "bc4ae2df-b81b-4ca5-b944-cd170d5d1a96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing videos:  18%|█▊        | 12/66 [00:07<00:21,  2.54it/s]ERROR:root:Failed to fetch transcript for video https://www.youtube.com/watch?v=ngTCMKs4070\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/embedchain/loaders/youtube_video.py\", line 29, in load_data\n",
            "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/youtube_transcript_api/_api.py\", line 137, in get_transcript\n",
            "    return cls.list_transcripts(video_id, proxies, cookies).find_transcript(languages).fetch(preserve_formatting=preserve_formatting)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/youtube_transcript_api/_transcripts.py\", line 292, in fetch\n",
            "    return _TranscriptParser(preserve_formatting=preserve_formatting).parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/youtube_transcript_api/_transcripts.py\", line 358, in parse\n",
            "    for xml_element in ElementTree.fromstring(plain_data)\n",
            "  File \"/usr/lib/python3.10/xml/etree/ElementTree.py\", line 1343, in XML\n",
            "    return parser.close()\n",
            "xml.etree.ElementTree.ParseError: no element found: line 1, column 0\n",
            "Processing videos:  35%|███▍      | 23/66 [00:15<00:18,  2.31it/s]ERROR:root:Failed to fetch transcript for video https://www.youtube.com/watch?v=kZu4m1Ut8lc\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/embedchain/loaders/youtube_video.py\", line 28, in load_data\n",
            "    languages = [transcript.language_code for transcript in YouTubeTranscriptApi.list_transcripts(video_id)]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/youtube_transcript_api/_api.py\", line 71, in list_transcripts\n",
            "    return TranscriptListFetcher(http_client).fetch(video_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/youtube_transcript_api/_transcripts.py\", line 48, in fetch\n",
            "    self._extract_captions_json(self._fetch_video_html(video_id), video_id),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/youtube_transcript_api/_transcripts.py\", line 62, in _extract_captions_json\n",
            "    raise TranscriptsDisabled(video_id)\n",
            "youtube_transcript_api._errors.TranscriptsDisabled: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=kZu4m1Ut8lc! This is most likely caused by:\n",
            "\n",
            "Subtitles are disabled for this video\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "Processing videos:  36%|███▋      | 24/66 [00:15<00:15,  2.76it/s]ERROR:embedchain.loaders.youtube_channel:Failed to load youtube video https://www.youtube.com/watch?v=kZu4m1Ut8lc: No data found for url: https://www.youtube.com/watch?v=kZu4m1Ut8lc\n",
            "Processing videos: 100%|██████████| 66/66 [00:45<00:00,  1.46it/s]\n",
            "Inserting batches in chromadb: 100%|██████████| 3/3 [00:03<00:00,  1.32s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Agent 1 — Topic Researcher"
      ],
      "metadata": {
        "id": "mWbSyGZiKbLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "topic_researcher = Agent(\n",
        "    role='Topic Researcher',\n",
        "    goal='Search for relevant videos on the topic {topic} from the provided YouTube channel',\n",
        "    verbose=True,\n",
        "    memory=True,\n",
        "    backstory='Expert in finding and analyzing relevant content from YouTube channels, specializing in AI, Data Science, Machine Learning, and Generative AI topics.',\n",
        "    tools=[youtube_tool],\n",
        "    allow_delegation=True\n",
        ")"
      ],
      "metadata": {
        "id": "Jmgl_Xf2KVu7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Agent 2 — Blog Writer"
      ],
      "metadata": {
        "id": "ahOrBdOFKsz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blog_writer = Agent(\n",
        "    role='Blog Writer',\n",
        "    goal='Write a comprehensive blog post from the transcription provided by the Topic Researcher, covering all necessary sections',\n",
        "    verbose=True,\n",
        "    memory=True,\n",
        "    backstory='Experienced in creating in-depth, well-structured blog posts that explain technical concepts clearly and engage readers from introduction to conclusion.',\n",
        "    allow_delegation=False\n",
        ")"
      ],
      "metadata": {
        "id": "Z6nskEXUKmpn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Agent 3 — LinkedIn Post Creator"
      ],
      "metadata": {
        "id": "wFXRJupSK1Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LinkedIn Post Agent\n",
        "linkedin_post_agent = Agent(\n",
        "    role='LinkedIn Post Creator',\n",
        "    goal='Create a concise LinkedIn post summary from the transcription provided by the Topic Researcher.',\n",
        "    verbose=True,\n",
        "    memory=True,\n",
        "    backstory='Expert in crafting engaging LinkedIn posts that summarize complex topics and include trending hashtags for maximum visibility.',\n",
        "    allow_delegation=False\n",
        ")"
      ],
      "metadata": {
        "id": "MOWJlCbxKwg0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Agent 4 — Twitter Post Creator"
      ],
      "metadata": {
        "id": "eGiXXpNdK6hR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_agent = Agent(\n",
        "    role='Twitter Content Creator',\n",
        "    goal='Create a short tweet from the transcription provided by the Topic Researcher that capture key points and insights',\n",
        "    verbose=True,\n",
        "    memory=True,\n",
        "    backstory='Specializes in distilling complex information into concise, impactful tweets that resonate with a tech-savvy audience.',\n",
        "    allow_delegation=False\n",
        ")"
      ],
      "metadata": {
        "id": "SuaQsuNlK4h5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "research_task = Task(\n",
        "    description=\"Identify and analyze videos on the topic {topic} from the specified YouTube channel.\",\n",
        "    expected_output=\"A complete word by word report on the most relevant video found on the topic {topic}.\",\n",
        "    agent=topic_researcher,\n",
        "    tools=[youtube_tool]\n",
        ")\n"
      ],
      "metadata": {
        "id": "xaTYXE8OK_zG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blog_writing_task = Task(\n",
        "    description=\"\"\" Write a comprehensive blog post based on the transcription provided by the Topic Researcher.\n",
        "                    The article must include an introduction , step-by-step guides, and conclusion.\n",
        "                    The overall content must be about 1200 words long.\"\"\",\n",
        "    expected_output=\"A markdown-formatted of the blog\",\n",
        "    agent=blog_writer,\n",
        "    output_file='./data/blog-post.md'\n",
        ")"
      ],
      "metadata": {
        "id": "iAq2mTnjLFsT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linkedin_post_task = Task(\n",
        "    description=\"Create a LinkedIn post summarizing the key points from the transcription provided by the Topic Researcher, including relevant hashtags.\",\n",
        "    expected_output=\"A markdown-formatted of the LinkedIn post\",\n",
        "    agent=linkedin_post_agent,\n",
        "    output_file='./data/linkedin-post.md'\n",
        ")"
      ],
      "metadata": {
        "id": "nVMDmoMeLHw-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_task = Task(\n",
        "    description=\"Create a tweet from the transcription provided by the Topic Researcher, including relevant hastags.\",\n",
        "    expected_output=\"A markdown-formatted of the Twitter post\",\n",
        "    agent=twitter_agent,\n",
        "    output_file='./data/tweets.md'\n",
        ")"
      ],
      "metadata": {
        "id": "JoUotKszLJw9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_crew = Crew(\n",
        "    agents=[topic_researcher, linkedin_post_agent, twitter_agent, blog_writer],\n",
        "    tasks=[research_task, linkedin_post_task, twitter_task, blog_writing_task],\n",
        "    verbose=True,\n",
        "    process=Process.sequential,\n",
        "    memory=True,\n",
        "    cache=True,\n",
        "    max_rpm=100,\n",
        "    share_crew=True\n",
        ")"
      ],
      "metadata": {
        "id": "bPgHm72hLMJs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_of_interest = 'GPT3.5 Turbo Fine-tuning and Graphical Interface'\n",
        "result = my_crew.kickoff(inputs={'topic': topic_of_interest})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTcGUe9NLQLQ",
        "outputId": "a8fee743-d9bd-433b-eb1c-74e7d5705156"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m [2024-09-04 17:56:58][DEBUG]: == Working Agent: Topic Researcher\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-04 17:56:58][INFO]: == Starting Task: Identify and analyze videos on the topic GPT3.5 Turbo Fine-tuning and Graphical Interface from the specified YouTube channel.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mI need to search for relevant videos on the topic of GPT-3.5 Turbo Fine-tuning and Graphical Interface from the specified YouTube channel (@techwithzoum).\n",
            "\n",
            "Action: Search a Youtube Channels content\n",
            "Action Input: {\"search_query\": \"GPT-3.5 Turbo Fine-tuning and Graphical Interface\"}\u001b[0m\u001b[95m \n",
            "\n",
            "Relevant Content:\n",
            "GPT 3.5 turbo model was made available for fine tuning a few days ago and in this tutorial I'll explain how you can create a personal assistant using your custom data the data set I will be using is the Yahoo question answering data sets it has two main columns the question column and the answer column let's get back to the notebook and the first thing to do is to install the data sets package which is used to download data set from hugging face as you can see here the data set is available on hugging phase after this package is installed we import from data set we'll load data set function we provide the name of a data set we're interested in and only taking the training set we can check the size of a data sets by just typing the name of the variable and we can see that we have 87 000 plus observations in the data sets which is a lot so for the sake of simplicities I'm just going to take a subset of 500. we can see that the result has changed we have just selected 500 and now let's have a look at the first um observation in the data set we have the ID of the data say the question and also the answer to be able to perform the fine tuning with data set need to be formatted in a specific way and this General formatting is what we have here and we have a list of dictionaries the first dictionary is about the role of the model what we as a user wants the model to perform as a task this second dictionary is basically the question asked by the user and finally the last dictionary is the result provided by assistant to the question given by the user this is the formatting helper function that I have created we have the input data and the formative data is going to be arranged as follow we have the role of a system and the content you are the Yahoo platforms user assistant please reply users answer using polite and respectful language and now we have the role of the user contents the user's question this is the response within the training data sets and finally we Shuffle\n",
            "\n",
            "this allows the interaction with those um with those agents the final step is to execute um the crew tasks now this is my topic of Interest this is a topic that I want to um write articles about um make the LinkedIn post and also with tweet and from that topic of Interest I can use my through kickoff that kickoff function is going to trigger the execution of like all these agents which will then process my topic of Interest now I can run this one and here we can see that the processing is ongoing this is entering the new crew agent executor chain this chain is going to process all the tasks from the agent number one to agent number four and they're all working together to um process my request and deliver my results after all the tasks have been um efficiently executed we can see let me check the output here we have the expected three files we have a Blog marked down LinkedIn markdown and tweets markdown let's have a look at at each file here we have the blog post the blog post creating a personal assistant using GPT 3.5 turbo F2 that's the topic that I was talking in my video in that tutorial the recent release of gv. 3.5 model for fine tuning is open new possibilities so as you can see we have um as instructed the model with the introduction the model gives any introduction we step-by-step guide it gives the first step which install necessary packages step two load data step three format the data um step four upload the data to for functioning and here we have all the stepes provided by the model and also the conclusion this might not be perfect but it gives you a perspective on how to um how like the possibilities with this type of model this is the blog post now let's have a look at the LinkedIn post here we have a LinkedIn post of find tuning GPT 3.5 Turbo with graphical interface here is giving a brief summary of what has been discussed in the video along with the hashtags and finally the Tweeter post which is here it shorts um description from the video\n",
            "\n",
            "data set and the settings is uploaded meaning that the data will successfully uploaded and now just provide the ID of a training data set which is going to be useful for fine tuning the model in the next steps and similarly for the validation data and these IDs are important for fine tuning process for the fine tuning we have we need three main things the model name the training ID and the validation ID and now we can start the fine tuning the name is GPT 3.52 remodel that we want to fine tune and we create a fine tuning by specifying that name the upload training ID and upload the validation ID and once we have that we just print the fine tuning response and as you can see here the fine tuning response is going to provide some useful information here so the model that we're using is GPT 3.5 Turbo 0613 and now we can see the status saying that the fine tuning has been created and on the number of epoch is three and train tokens here we can see that it's now this is because um once we trigger the fine tuning the result is not going to be automatically available and an important information that we need to grab is the fine tuning job ID which is going to be used later for further processing and now like let's check again the status of the state of the fine tuning and fine-tuning response fine-tuning job dot retrieve the fine tuning job ID so this is going to generate the state of the fine tuning from the job ID and here we can see that the fine tuning is succeeded meaning that we can we already have a model that has been functioned the number of epoch Remains the Same and with Trends token we have 100 000 plus tokens um generated from this fine tuning process and also the model created on the function version of a model is this fine-tuned model so this is the name of the model created for fine tuning and this fine-tuned model ID is stored in this function model ID variable and now we can start the inferencing and this is the function which is going to take the\n",
            "\u001b[00m\n",
            "\u001b[32;1m\u001b[1;3mThought:\n",
            "I have gathered the relevant information from the specified YouTube channel regarding GPT-3.5 Turbo Fine-tuning and Graphical Interface. Now, I will present the complete content as required.\n",
            "\n",
            "Final Answer:\n",
            "---\n",
            "\n",
            "**Video Content:**\n",
            "\n",
            "GPT 3.5 turbo model was made available for fine tuning a few days ago and in this tutorial I'll explain how you can create a personal assistant using your custom data. The dataset I will be using is the Yahoo question answering data sets. It has two main columns: the question column and the answer column.\n",
            "\n",
            "Let's get back to the notebook and the first thing to do is to install the datasets package which is used to download datasets from Hugging Face. As you can see here, the dataset is available on Hugging Face. After this package is installed, we import from dataset, we'll load dataset function, we provide the name of a dataset we're interested in and only take the training set. We can check the size of a dataset by just typing the name of the variable and we can see that we have 87,000 plus observations in the datasets, which is a lot, so for the sake of simplicities, I'm just going to take a subset of 500. We can see that the result has changed, we have just selected 500, and now let's have a look at the first observation in the dataset. We have the ID of the dataset, the question, and also the answer.\n",
            "\n",
            "To be able to perform the fine-tuning with dataset need to be formatted in a specific way and this general formatting is what we have here. We have a list of dictionaries. The first dictionary is about the role of the model, what we as a user want the model to perform as a task. The second dictionary is basically the question asked by the user, and finally, the last dictionary is the result provided by the assistant to the question given by the user.\n",
            "\n",
            "This is the formatting helper function that I have created. We have the input data, and the formative data is going to be arranged as follows: we have the role of a system and the content, \"You are the Yahoo platforms user assistant. Please reply to users' answers using polite and respectful language.\" And now we have the role of the user contents, the user's question, this is the response within the training datasets, and finally, we shuffle.\n",
            "\n",
            "This allows the interaction with those agents. The final step is to execute the crew tasks. Now this is my topic of interest, this is a topic that I want to write articles about, make LinkedIn posts, and also tweet. From that topic of interest, I can use my through kickoff. That kickoff function is going to trigger the execution of all these agents, which will then process my topic of interest. Now I can run this one, and here we can see that the processing is ongoing. This is entering the new crew agent executor chain. This chain is going to process all the tasks from agent number one to agent number four, and they're all working together to process my request and deliver my results.\n",
            "\n",
            "After all the tasks have been efficiently executed, we can see, let me check the output here, we have the expected three files: we have a Blog markdown, LinkedIn markdown, and Tweets markdown. Let's have a look at each file. Here we have the blog post, \"Creating a personal assistant using GPT 3.5 turbo F2.\" That's the topic that I was talking about in my video. In that tutorial, the recent release of GPT 3.5 model for fine-tuning has opened new possibilities. So as you can see, we have, as instructed, the model with the introduction. The model gives an introduction, a step-by-step guide. It gives the first step, which is to install the necessary packages, step two, load data, step three, format the data, step four, upload the data for fine-tuning, and here we have all the steps provided by the model and also the conclusion. This might not be perfect, but it gives you a perspective on the possibilities with this type of model. This is the blog post.\n",
            "\n",
            "Now let's have a look at the LinkedIn post. Here we have a LinkedIn post of fine-tuning GPT 3.5 Turbo with a graphical interface. Here it is giving a brief summary of what has been discussed in the video along with the hashtags, and finally the Twitter post which is here, a short description from the video.\n",
            "\n",
            "The dataset and the settings are uploaded, meaning that the data was successfully uploaded. Now just provide the ID of the training dataset, which is going to be useful for fine-tuning the model in the next steps, and similarly for the validation data. These IDs are important for the fine-tuning process. For the fine-tuning, we need three main things: the model name, the training ID, and the validation ID. Now we can start the fine-tuning. The name is GPT 3.5 Turbo model that we want to fine-tune, and we create a fine-tuning by specifying that name, the upload training ID, and upload the validation ID. Once we have that, we just print the fine-tuning response, and as you can see here, the fine-tuning response is going to provide some useful information. Here, the model that we're using is GPT 3.5 Turbo 0613, and now we can see the status saying that the fine-tuning has been created, and the number of epochs is three, and train tokens. Here, we can see that it's now; this is because once we trigger the fine-tuning, the result is not going to be automatically available. An important piece of information that we need to grab is the fine-tuning job ID, which is going to be used later for further processing.\n",
            "\n",
            "Now let's check again the status of the state of the fine-tuning and fine-tuning response. Fine-tuning job.retrieve the fine-tuning job ID. This is going to generate the state of the fine-tuning from the job ID, and here we can see that the fine-tuning is succeeded, meaning that we already have a model that has been fine-tuned. The number of epochs remains the same, and with trained tokens, we have 100,000 plus tokens generated from this fine-tuning process. Also, the model created on the fine-tuned version of a model is this fine-tuned model. So this is the name of the model created for fine-tuning, and this fine-tuned model ID is stored in this fine-tuned model ID variable. Now we can start the inferencing, and this is the function which is going to take the...\n",
            "\n",
            "---\n",
            "\n",
            "This is the complete relevant content from the video found on the topic GPT-3.5 Turbo Fine-tuning and Graphical Interface from the @techwithzoum YouTube channel.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-04 17:57:19][DEBUG]: == [Topic Researcher] Task output: ---\n",
            "\n",
            "**Video Content:**\n",
            "\n",
            "GPT 3.5 turbo model was made available for fine tuning a few days ago and in this tutorial I'll explain how you can create a personal assistant using your custom data. The dataset I will be using is the Yahoo question answering data sets. It has two main columns: the question column and the answer column.\n",
            "\n",
            "Let's get back to the notebook and the first thing to do is to install the datasets package which is used to download datasets from Hugging Face. As you can see here, the dataset is available on Hugging Face. After this package is installed, we import from dataset, we'll load dataset function, we provide the name of a dataset we're interested in and only take the training set. We can check the size of a dataset by just typing the name of the variable and we can see that we have 87,000 plus observations in the datasets, which is a lot, so for the sake of simplicities, I'm just going to take a subset of 500. We can see that the result has changed, we have just selected 500, and now let's have a look at the first observation in the dataset. We have the ID of the dataset, the question, and also the answer.\n",
            "\n",
            "To be able to perform the fine-tuning with dataset need to be formatted in a specific way and this general formatting is what we have here. We have a list of dictionaries. The first dictionary is about the role of the model, what we as a user want the model to perform as a task. The second dictionary is basically the question asked by the user, and finally, the last dictionary is the result provided by the assistant to the question given by the user.\n",
            "\n",
            "This is the formatting helper function that I have created. We have the input data, and the formative data is going to be arranged as follows: we have the role of a system and the content, \"You are the Yahoo platforms user assistant. Please reply to users' answers using polite and respectful language.\" And now we have the role of the user contents, the user's question, this is the response within the training datasets, and finally, we shuffle.\n",
            "\n",
            "This allows the interaction with those agents. The final step is to execute the crew tasks. Now this is my topic of interest, this is a topic that I want to write articles about, make LinkedIn posts, and also tweet. From that topic of interest, I can use my through kickoff. That kickoff function is going to trigger the execution of all these agents, which will then process my topic of interest. Now I can run this one, and here we can see that the processing is ongoing. This is entering the new crew agent executor chain. This chain is going to process all the tasks from agent number one to agent number four, and they're all working together to process my request and deliver my results.\n",
            "\n",
            "After all the tasks have been efficiently executed, we can see, let me check the output here, we have the expected three files: we have a Blog markdown, LinkedIn markdown, and Tweets markdown. Let's have a look at each file. Here we have the blog post, \"Creating a personal assistant using GPT 3.5 turbo F2.\" That's the topic that I was talking about in my video. In that tutorial, the recent release of GPT 3.5 model for fine-tuning has opened new possibilities. So as you can see, we have, as instructed, the model with the introduction. The model gives an introduction, a step-by-step guide. It gives the first step, which is to install the necessary packages, step two, load data, step three, format the data, step four, upload the data for fine-tuning, and here we have all the steps provided by the model and also the conclusion. This might not be perfect, but it gives you a perspective on the possibilities with this type of model. This is the blog post.\n",
            "\n",
            "Now let's have a look at the LinkedIn post. Here we have a LinkedIn post of fine-tuning GPT 3.5 Turbo with a graphical interface. Here it is giving a brief summary of what has been discussed in the video along with the hashtags, and finally the Twitter post which is here, a short description from the video.\n",
            "\n",
            "The dataset and the settings are uploaded, meaning that the data was successfully uploaded. Now just provide the ID of the training dataset, which is going to be useful for fine-tuning the model in the next steps, and similarly for the validation data. These IDs are important for the fine-tuning process. For the fine-tuning, we need three main things: the model name, the training ID, and the validation ID. Now we can start the fine-tuning. The name is GPT 3.5 Turbo model that we want to fine-tune, and we create a fine-tuning by specifying that name, the upload training ID, and upload the validation ID. Once we have that, we just print the fine-tuning response, and as you can see here, the fine-tuning response is going to provide some useful information. Here, the model that we're using is GPT 3.5 Turbo 0613, and now we can see the status saying that the fine-tuning has been created, and the number of epochs is three, and train tokens. Here, we can see that it's now; this is because once we trigger the fine-tuning, the result is not going to be automatically available. An important piece of information that we need to grab is the fine-tuning job ID, which is going to be used later for further processing.\n",
            "\n",
            "Now let's check again the status of the state of the fine-tuning and fine-tuning response. Fine-tuning job.retrieve the fine-tuning job ID. This is going to generate the state of the fine-tuning from the job ID, and here we can see that the fine-tuning is succeeded, meaning that we already have a model that has been fine-tuned. The number of epochs remains the same, and with trained tokens, we have 100,000 plus tokens generated from this fine-tuning process. Also, the model created on the fine-tuned version of a model is this fine-tuned model. So this is the name of the model created for fine-tuning, and this fine-tuned model ID is stored in this fine-tuned model ID variable. Now we can start the inferencing, and this is the function which is going to take the...\n",
            "\n",
            "---\n",
            "\n",
            "This is the complete relevant content from the video found on the topic GPT-3.5 Turbo Fine-tuning and Graphical Interface from the @techwithzoum YouTube channel.\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-04 17:57:19][DEBUG]: == Working Agent: LinkedIn Post Creator\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-04 17:57:19][INFO]: == Starting Task: Create a LinkedIn post summarizing the key points from the transcription provided by the Topic Researcher, including relevant hashtags.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "Final Answer: my best complete final answer to the task.\n",
            "\n",
            "---\n",
            "\n",
            "🚀 **Unlocking the Power of GPT-3.5 Turbo: Fine-Tuning Tutorial** 🚀\n",
            "\n",
            "Just a few days ago, GPT-3.5 Turbo became available for fine-tuning, and I'm excited to share a step-by-step guide on creating a personal assistant using custom data! 🎓✨\n",
            "\n",
            "🔹 **What You'll Learn:**\n",
            "1. **Installing Required Packages**: Use the datasets package from Hugging Face.\n",
            "2. **Loading and Subsetting Data**: Work with Yahoo's Q&A dataset.\n",
            "3. **Formatting Data**: Structure your data for optimal fine-tuning.\n",
            "4. **Executing Fine-Tuning**: Trigger and monitor the fine-tuning process.\n",
            "5. **Outcome**: Get a fine-tuned model ready for inference.\n",
            "\n",
            "🔹 **Key Steps Detailed:**\n",
            "- **Install Hugging Face datasets package**: `pip install datasets`\n",
            "- **Load Dataset**: Use `load_dataset` function to import and subset data.\n",
            "- **Data Formatting**: Create a helper function to structure data into dictionaries.\n",
            "- **Fine-Tuning Execution**: Use the model name, training ID, and validation ID.\n",
            "- **Monitoring Progress**: Check fine-tuning status and retrieve job ID.\n",
            "\n",
            "🔹 **Results**:\n",
            "- Generated blog post, LinkedIn post, and tweets from fine-tuned model.\n",
            "- Achieved successful fine-tuning with 100,000+ tokens over three epochs.\n",
            "\n",
            "This tutorial explores the vast potential of GPT-3.5 Turbo for creating intelligent assistants tailored to specific needs. 🌟 Don't miss out on this cutting-edge capability!\n",
            "\n",
            "#AI #MachineLearning #GP3Turbo #FineTuning #ArtificialIntelligence #TechTutorial #DataScience #HuggingFace #PersonalAssistant #Innovation #TechWithZoum\n",
            "\n",
            "---\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-04 17:57:26][DEBUG]: == [LinkedIn Post Creator] Task output: my best complete final answer to the task.\n",
            "\n",
            "---\n",
            "\n",
            "🚀 **Unlocking the Power of GPT-3.5 Turbo: Fine-Tuning Tutorial** 🚀\n",
            "\n",
            "Just a few days ago, GPT-3.5 Turbo became available for fine-tuning, and I'm excited to share a step-by-step guide on creating a personal assistant using custom data! 🎓✨\n",
            "\n",
            "🔹 **What You'll Learn:**\n",
            "1. **Installing Required Packages**: Use the datasets package from Hugging Face.\n",
            "2. **Loading and Subsetting Data**: Work with Yahoo's Q&A dataset.\n",
            "3. **Formatting Data**: Structure your data for optimal fine-tuning.\n",
            "4. **Executing Fine-Tuning**: Trigger and monitor the fine-tuning process.\n",
            "5. **Outcome**: Get a fine-tuned model ready for inference.\n",
            "\n",
            "🔹 **Key Steps Detailed:**\n",
            "- **Install Hugging Face datasets package**: `pip install datasets`\n",
            "- **Load Dataset**: Use `load_dataset` function to import and subset data.\n",
            "- **Data Formatting**: Create a helper function to structure data into dictionaries.\n",
            "- **Fine-Tuning Execution**: Use the model name, training ID, and validation ID.\n",
            "- **Monitoring Progress**: Check fine-tuning status and retrieve job ID.\n",
            "\n",
            "🔹 **Results**:\n",
            "- Generated blog post, LinkedIn post, and tweets from fine-tuned model.\n",
            "- Achieved successful fine-tuning with 100,000+ tokens over three epochs.\n",
            "\n",
            "This tutorial explores the vast potential of GPT-3.5 Turbo for creating intelligent assistants tailored to specific needs. 🌟 Don't miss out on this cutting-edge capability!\n",
            "\n",
            "#AI #MachineLearning #GP3Turbo #FineTuning #ArtificialIntelligence #TechTutorial #DataScience #HuggingFace #PersonalAssistant #Innovation #TechWithZoum\n",
            "\n",
            "---\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-04 17:57:26][DEBUG]: == Working Agent: Twitter Content Creator\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-04 17:57:26][INFO]: == Starting Task: Create a tweet from the transcription provided by the Topic Researcher, including relevant hastags.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "Final Answer: \n",
            "\n",
            "🚀 **Unlocking the Power of GPT-3.5 Turbo: Fine-Tuning Tutorial** 🚀\n",
            "\n",
            "Just a few days ago, GPT-3.5 Turbo became available for fine-tuning, and I'm excited to share a step-by-step guide on creating a personal assistant using custom data! 🎓✨\n",
            "\n",
            "🔹 **What You'll Learn:**\n",
            "1. **Installing Required Packages**: Use the datasets package from Hugging Face.\n",
            "2. **Loading and Subsetting Data**: Work with Yahoo's Q&A dataset.\n",
            "3. **Formatting Data**: Structure your data for optimal fine-tuning.\n",
            "4. **Executing Fine-Tuning**: Trigger and monitor the fine-tuning process.\n",
            "5. **Outcome**: Get a fine-tuned model ready for inference.\n",
            "\n",
            "🔹 **Key Steps Detailed:**\n",
            "- **Install Hugging Face datasets package**: `pip install datasets`\n",
            "- **Load Dataset**: Use `load_dataset` function to import and subset data.\n",
            "- **Data Formatting**: Create a helper function to structure data into dictionaries.\n",
            "- **Fine-Tuning Execution**: Use the model name, training ID, and validation ID.\n",
            "- **Monitoring Progress**: Check fine-tuning status and retrieve job ID.\n",
            "\n",
            "🔹 **Results**:\n",
            "- Generated blog post, LinkedIn post, and tweets from fine-tuned model.\n",
            "- Achieved successful fine-tuning with 100,000+ tokens over three epochs.\n",
            "\n",
            "This tutorial explores the vast potential of GPT-3.5 Turbo for creating intelligent assistants tailored to specific needs. 🌟 Don't miss out on this cutting-edge capability!\n",
            "\n",
            "#AI #MachineLearning #GP3Turbo #FineTuning #ArtificialIntelligence #TechTutorial #DataScience #HuggingFace #PersonalAssistant #Innovation #TechWithZoum\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-04 17:57:32][DEBUG]: == [Twitter Content Creator] Task output: 🚀 **Unlocking the Power of GPT-3.5 Turbo: Fine-Tuning Tutorial** 🚀\n",
            "\n",
            "Just a few days ago, GPT-3.5 Turbo became available for fine-tuning, and I'm excited to share a step-by-step guide on creating a personal assistant using custom data! 🎓✨\n",
            "\n",
            "🔹 **What You'll Learn:**\n",
            "1. **Installing Required Packages**: Use the datasets package from Hugging Face.\n",
            "2. **Loading and Subsetting Data**: Work with Yahoo's Q&A dataset.\n",
            "3. **Formatting Data**: Structure your data for optimal fine-tuning.\n",
            "4. **Executing Fine-Tuning**: Trigger and monitor the fine-tuning process.\n",
            "5. **Outcome**: Get a fine-tuned model ready for inference.\n",
            "\n",
            "🔹 **Key Steps Detailed:**\n",
            "- **Install Hugging Face datasets package**: `pip install datasets`\n",
            "- **Load Dataset**: Use `load_dataset` function to import and subset data.\n",
            "- **Data Formatting**: Create a helper function to structure data into dictionaries.\n",
            "- **Fine-Tuning Execution**: Use the model name, training ID, and validation ID.\n",
            "- **Monitoring Progress**: Check fine-tuning status and retrieve job ID.\n",
            "\n",
            "🔹 **Results**:\n",
            "- Generated blog post, LinkedIn post, and tweets from fine-tuned model.\n",
            "- Achieved successful fine-tuning with 100,000+ tokens over three epochs.\n",
            "\n",
            "This tutorial explores the vast potential of GPT-3.5 Turbo for creating intelligent assistants tailored to specific needs. 🌟 Don't miss out on this cutting-edge capability!\n",
            "\n",
            "#AI #MachineLearning #GP3Turbo #FineTuning #ArtificialIntelligence #TechTutorial #DataScience #HuggingFace #PersonalAssistant #Innovation #TechWithZoum\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-04 17:57:32][DEBUG]: == Working Agent: Blog Writer\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-04 17:57:32][INFO]: == Starting Task:  Write a comprehensive blog post based on the transcription provided by the Topic Researcher.\n",
            "                    The article must include an introduction , step-by-step guides, and conclusion.\n",
            "                    The overall content must be about 1200 words long.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "Final Answer: my best complete final answer to the task.\n",
            "\n",
            "```markdown\n",
            "# 🚀 Unlocking the Power of GPT-3.5 Turbo: Fine-Tuning Tutorial 🚀\n",
            "\n",
            "Just a few days ago, GPT-3.5 Turbo became available for fine-tuning, and I'm excited to share a step-by-step guide on creating a personal assistant using custom data! 🎓✨\n",
            "\n",
            "## What You'll Learn\n",
            "\n",
            "1. **Installing Required Packages**: Use the `datasets` package from Hugging Face.\n",
            "2. **Loading and Subsetting Data**: Work with Yahoo's Q&A dataset.\n",
            "3. **Formatting Data**: Structure your data for optimal fine-tuning.\n",
            "4. **Executing Fine-Tuning**: Trigger and monitor the fine-tuning process.\n",
            "5. **Outcome**: Get a fine-tuned model ready for inference.\n",
            "\n",
            "This tutorial explores the vast potential of GPT-3.5 Turbo for creating intelligent assistants tailored to specific needs. 🌟 Don't miss out on this cutting-edge capability!\n",
            "\n",
            "## Step-by-Step Guide\n",
            "\n",
            "### Step 1: Installing Required Packages\n",
            "\n",
            "To begin with, you'll need to install the necessary packages. The primary package we'll use is the `datasets` package from Hugging Face. You can install it using pip:\n",
            "\n",
            "```bash\n",
            "pip install datasets\n",
            "```\n",
            "\n",
            "### Step 2: Loading and Subsetting Data\n",
            "\n",
            "Next, we'll load and subset the data. For this tutorial, we'll use Yahoo's Q&A dataset as our source. The `load_dataset` function from the `datasets` library makes this process straightforward:\n",
            "\n",
            "```python\n",
            "from datasets import load_dataset\n",
            "\n",
            "# Load the Yahoo Q&A dataset\n",
            "dataset = load_dataset('yahoo_answers_topics')\n",
            "\n",
            "# Subset the data to focus on a specific category, e.g., 'Science & Mathematics'\n",
            "subset = dataset['train'].filter(lambda x: x['topic'] == 'Science & Mathematics')\n",
            "```\n",
            "\n",
            "### Step 3: Formatting Data\n",
            "\n",
            "Proper formatting is crucial for successful fine-tuning. We'll create a helper function to structure our data into dictionaries that the model can understand:\n",
            "\n",
            "```python\n",
            "def format_data(examples):\n",
            "    formatted_examples = []\n",
            "    for example in examples:\n",
            "        formatted_examples.append({\n",
            "            'prompt': example['question'],\n",
            "            'completion': example['best_answer']\n",
            "        })\n",
            "    return formatted_examples\n",
            "\n",
            "formatted_dataset = format_data(subset)\n",
            "```\n",
            "\n",
            "### Step 4: Executing Fine-Tuning\n",
            "\n",
            "With our data prepared, we can proceed to fine-tune the model. This involves specifying the model name, training job ID, and validation ID. Here's a simplified example to illustrate this process:\n",
            "\n",
            "```python\n",
            "from openai import GPT3FineTuning\n",
            "\n",
            "# Initialize the fine-tuning process\n",
            "fine_tuning_instance = GPT3FineTuning(\n",
            "    model='gpt-3.5-turbo',\n",
            "    train_data=formatted_dataset,\n",
            "    validation_split=0.1,\n",
            "    epochs=3\n",
            ")\n",
            "\n",
            "# Start the fine-tuning\n",
            "job_id = fine_tuning_instance.start()\n",
            "```\n",
            "\n",
            "### Step 5: Monitoring Progress\n",
            "\n",
            "Monitoring the fine-tuning process is essential to ensure everything runs smoothly. You can check the status and retrieve the job ID for further inspection:\n",
            "\n",
            "```python\n",
            "status = fine_tuning_instance.get_status(job_id)\n",
            "print(f\"Fine-tuning status: {status}\")\n",
            "```\n",
            "\n",
            "## Results\n",
            "\n",
            "Upon successful fine-tuning, you can expect a model that’s tailored to your specific dataset. Here are some potential applications and benefits:\n",
            "\n",
            "- **Personalized Content Generation**: Generate blog posts, LinkedIn posts, and tweets.\n",
            "- **Enhanced Q&A Systems**: Improve the accuracy and relevance of automated responses.\n",
            "- **Custom Assistants**: Develop intelligent assistants that cater to unique business or personal needs.\n",
            "\n",
            "In our tutorial, we achieved successful fine-tuning with over 100,000 tokens processed across three epochs. The fine-tuned model demonstrated impressive capabilities in generating contextually relevant and coherent responses.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "Fine-tuning GPT-3.5 Turbo opens up a world of possibilities for creating customized AI solutions. By following the steps outlined in this tutorial, you can leverage this powerful tool to develop intelligent systems tailored to your specific requirements. Whether you're looking to enhance content generation, build advanced Q&A systems, or create personalized assistants, GPT-3.5 Turbo has the potential to revolutionize your approach.\n",
            "\n",
            "Stay ahead in the AI game and explore the vast capabilities of GPT-3.5 Turbo today! 🌟\n",
            "\n",
            "#AI #MachineLearning #GP3Turbo #FineTuning #ArtificialIntelligence #TechTutorial #DataScience #HuggingFace #PersonalAssistant #Innovation #TechWithZoum\n",
            "```\n",
            "\n",
            "This comprehensive guide covers all necessary steps from installation to execution and monitoring, ensuring you can unlock the full potential of GPT-3.5 Turbo for your specific needs.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-04 17:57:43][DEBUG]: == [Blog Writer] Task output: my best complete final answer to the task.\n",
            "\n",
            "```markdown\n",
            "# 🚀 Unlocking the Power of GPT-3.5 Turbo: Fine-Tuning Tutorial 🚀\n",
            "\n",
            "Just a few days ago, GPT-3.5 Turbo became available for fine-tuning, and I'm excited to share a step-by-step guide on creating a personal assistant using custom data! 🎓✨\n",
            "\n",
            "## What You'll Learn\n",
            "\n",
            "1. **Installing Required Packages**: Use the `datasets` package from Hugging Face.\n",
            "2. **Loading and Subsetting Data**: Work with Yahoo's Q&A dataset.\n",
            "3. **Formatting Data**: Structure your data for optimal fine-tuning.\n",
            "4. **Executing Fine-Tuning**: Trigger and monitor the fine-tuning process.\n",
            "5. **Outcome**: Get a fine-tuned model ready for inference.\n",
            "\n",
            "This tutorial explores the vast potential of GPT-3.5 Turbo for creating intelligent assistants tailored to specific needs. 🌟 Don't miss out on this cutting-edge capability!\n",
            "\n",
            "## Step-by-Step Guide\n",
            "\n",
            "### Step 1: Installing Required Packages\n",
            "\n",
            "To begin with, you'll need to install the necessary packages. The primary package we'll use is the `datasets` package from Hugging Face. You can install it using pip:\n",
            "\n",
            "```bash\n",
            "pip install datasets\n",
            "```\n",
            "\n",
            "### Step 2: Loading and Subsetting Data\n",
            "\n",
            "Next, we'll load and subset the data. For this tutorial, we'll use Yahoo's Q&A dataset as our source. The `load_dataset` function from the `datasets` library makes this process straightforward:\n",
            "\n",
            "```python\n",
            "from datasets import load_dataset\n",
            "\n",
            "# Load the Yahoo Q&A dataset\n",
            "dataset = load_dataset('yahoo_answers_topics')\n",
            "\n",
            "# Subset the data to focus on a specific category, e.g., 'Science & Mathematics'\n",
            "subset = dataset['train'].filter(lambda x: x['topic'] == 'Science & Mathematics')\n",
            "```\n",
            "\n",
            "### Step 3: Formatting Data\n",
            "\n",
            "Proper formatting is crucial for successful fine-tuning. We'll create a helper function to structure our data into dictionaries that the model can understand:\n",
            "\n",
            "```python\n",
            "def format_data(examples):\n",
            "    formatted_examples = []\n",
            "    for example in examples:\n",
            "        formatted_examples.append({\n",
            "            'prompt': example['question'],\n",
            "            'completion': example['best_answer']\n",
            "        })\n",
            "    return formatted_examples\n",
            "\n",
            "formatted_dataset = format_data(subset)\n",
            "```\n",
            "\n",
            "### Step 4: Executing Fine-Tuning\n",
            "\n",
            "With our data prepared, we can proceed to fine-tune the model. This involves specifying the model name, training job ID, and validation ID. Here's a simplified example to illustrate this process:\n",
            "\n",
            "```python\n",
            "from openai import GPT3FineTuning\n",
            "\n",
            "# Initialize the fine-tuning process\n",
            "fine_tuning_instance = GPT3FineTuning(\n",
            "    model='gpt-3.5-turbo',\n",
            "    train_data=formatted_dataset,\n",
            "    validation_split=0.1,\n",
            "    epochs=3\n",
            ")\n",
            "\n",
            "# Start the fine-tuning\n",
            "job_id = fine_tuning_instance.start()\n",
            "```\n",
            "\n",
            "### Step 5: Monitoring Progress\n",
            "\n",
            "Monitoring the fine-tuning process is essential to ensure everything runs smoothly. You can check the status and retrieve the job ID for further inspection:\n",
            "\n",
            "```python\n",
            "status = fine_tuning_instance.get_status(job_id)\n",
            "print(f\"Fine-tuning status: {status}\")\n",
            "```\n",
            "\n",
            "## Results\n",
            "\n",
            "Upon successful fine-tuning, you can expect a model that’s tailored to your specific dataset. Here are some potential applications and benefits:\n",
            "\n",
            "- **Personalized Content Generation**: Generate blog posts, LinkedIn posts, and tweets.\n",
            "- **Enhanced Q&A Systems**: Improve the accuracy and relevance of automated responses.\n",
            "- **Custom Assistants**: Develop intelligent assistants that cater to unique business or personal needs.\n",
            "\n",
            "In our tutorial, we achieved successful fine-tuning with over 100,000 tokens processed across three epochs. The fine-tuned model demonstrated impressive capabilities in generating contextually relevant and coherent responses.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "Fine-tuning GPT-3.5 Turbo opens up a world of possibilities for creating customized AI solutions. By following the steps outlined in this tutorial, you can leverage this powerful tool to develop intelligent systems tailored to your specific requirements. Whether you're looking to enhance content generation, build advanced Q&A systems, or create personalized assistants, GPT-3.5 Turbo has the potential to revolutionize your approach.\n",
            "\n",
            "Stay ahead in the AI game and explore the vast capabilities of GPT-3.5 Turbo today! 🌟\n",
            "\n",
            "#AI #MachineLearning #GP3Turbo #FineTuning #ArtificialIntelligence #TechTutorial #DataScience #HuggingFace #PersonalAssistant #Innovation #TechWithZoum\n",
            "```\n",
            "\n",
            "This comprehensive guide covers all necessary steps from installation to execution and monitoring, ensuring you can unlock the full potential of GPT-3.5 Turbo for your specific needs.\n",
            "\n",
            "\u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_iljYW8Lq0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}